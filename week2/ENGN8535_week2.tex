\documentclass{article}

% add preamable: packages
\usepackage{amsmath} % remove the equation numbering
\usepackage{amssymb} %maths
\usepackage{graphicx}

\title{Week 2 Reading Notes}
\date{2020-03-05}
\author{Hongjian He}

\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \pagenumbering{arabic}

\section{Linear Algebra}

We are considering the matrix multiplication: an associative operation on M. Then distributive
law and associative law holds.

\subsection{Norms}

The Euclidean ($l_2$) or in general, $l_n$ norm:

$$
||x||_2 = (\sum_{i=1}^n x_i^n)^{\frac{1}{n}}
$$

And Frobenius Norm for matrix:

$$
||A||_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n A_{ij}^2} =\sqrt{tr(A^TA)}
$$

\subsection{Linear Independence}

Two vectors in $\mathbb{R}^n$ is orthogonal if there inner product is 0. A vector is normalised 
if its $l_2$ norm is 1. A square matrix is orthogonal if all its column vectors are pair-wise 
orthogonal and are normalised.

Rank theorem:

$$
R(A^T)\cap N(A)=\emptyset
$$

and

$$
Span\{(A^T),N(A)\}=\mathbb{R}^n
$$

\subsection{Quadratic form}

The map $x^TAx$ from $\mathbb{R}^n \times \mathbb{R}^n$ to $\mathbb{R}$ is called quadratic form.

So A is called positive definite if its quadratic form is greater than 0 and A is symmetric.



\section{Probability Theory}

\subsection{foundations of probability}

There are 3 components of a probability theory

\begin{enumerate}
\item Sample space $\Omega$
\item $\sigma$ algebra $\mathbb{F}$ on $\Omega$ to form a measurable space $(\Omega,\mathbb{F})$
\item A mapping $P: \mathbb{F}\to\mathbb{R}$ such that:
	\begin{enumerate}
	\item $(\forall A\in\mathbb{F})P(A)\ge 0$
	\item $P(\Omega)=1$
	\item $P(\cup_{k_1}^\infty A_k)=\sum_{k=1}^\infty P(A_k)$ if the family of subsets of $\mathbb{F}$ are mutually disjoint
	\end{enumerate}

\end{enumerate}

The consequences of these are well-understood relationship between set operation and 
real field.

Define the conditional probability of event A given B:

$$
P(A|B) = \frac{P(A\cap B)}{P(B)} 
$$

, and because of commutativeness of intersection, we have Bayes' Rule.
The intuition is based on the fact that what is the proportion of "amount of measure" 
of A restricted to B.


The linear independence is defined by $A\bot B$ if $P(A\cap B)=P(A)P(B)$

\subsection{Random Variables}

\textbf{Definition}: A random variable is a function from an event space $(\Omega,\mathbb{F})$
to another event space $(\Omega^\prime,\mathbb{F}^\prime)$ such that for every member
in $\mathbb{F}^\prime$, its inverse image under the function is a member in $\mathbb{F}$

The discrete/continuous random variable is then defined by their cumulative distribution
function $F_X: \mathbb{R}\to[0,1]: F_X(x)=P(X\le x)$, by generating sets of Borel sets.

If the CDF is differentiable, then it has PDF; otherwise it has PMF.

\subsection{Summary Statistics}

\textbf{Definition}: Expectation for a random variable $X$ under a function
$g:\mathbb{R}\to \mathbb{R}$
 is a "weighted average" of 
it with respect to its distribution:

$$
E[g(X)] = \sum_{x\in Val(X)} g(x)p_X(x)
$$

or


$$
E[g(X)] = \int_{-\infty}^\infty g(x)p_X(x)
$$

\textbf{Definition}: Variance of a random variable X is:

$$
Var[X] = E[(X-E[X])^2]=E[X^2]-E[X]^2
$$


For two variable case, the expectation is defined in the obvious way:

$$
E[g(X,Y)] = \sum_{x\in Var(X),y\in Val(Y)}g(x,y)p_{XY}(x,y)
$$

$$
E[g(X,Y)] = \int_{-\infty}^\infty\int_{-\infty}^\infty g(x,y)p_{XY}(x,y)dxdy
$$

And define the covariance is 

$$
Cov[X,Y] = E[(X-E[X])(Y-E[Y])] = E[XY]-E[X]E[Y]
$$

Now we can define the random vector in the usual way. And the covariance matrix is:

$$
\Sigma = E[XX^T]-E[X]E[X]^T= E[(X-E[X])(X-E[X])^T]
$$

i.e. the pair-wise covariance of all $x_i$.


\section{Ranking and Recommendation}

\subsection{Google search system}
It makes a lot of money by performing the first hyperlink analysis and create
the content-based search engines.

The idea is to assigning a score to billions of web pages.

\subsubsection{Problem definition}

Given a directed graph, which are the most important nodes?

It is equivalant to given a random walker on the nodes, what are the 
steady-state probabilities of each node.

Markov chain model describes it as Eigenvalue problem of the transition
probability matrix, plus some random jump.


Consider the recursive definition of rank of a page P:

$$
r(P) = \sum_{Q\in B_P} \frac{r(Q)}{|Q|}
$$

,where $B_P$ is the set of all "parent" pages that links to $P$,
and $|Q|$ is the number of links from page $Q$ to all other pages. The idea 
is that the importance of parent infers the importance of children, up 
to some proportion.

Now if we allow enumeration of all such pages, and an updating method on rank:

$$
r_k (P_i) =  \sum_{Q\in B_{P_i}} \frac{r_{k-1}(Q)}{|Q|}
$$

, where k is the updating step, and i is the index of page.

Now the page rank vector is simply a vector of rank of pages:

$$
v_k = [r_k(P_1),r_k(P_2),...,r_k(P_n)]^T
$$

and the transition from k-1 to k is:

$$
v_k = Hv_{k-1}
$$

,where $H_{ij}=\frac{1}{|P_i|}$ if $P_i$ links to $P_j$ and 0 otherwise.

Iterating this update equation is called power method. So the PageRank
vector is $lim_{k\to\infty}v_k$

The Google Matrix:

$$
G = \alpha S+ (1-\alpha)ue^T
$$
,where $\alpha\in (0,1)$, $S=H+au^T$ (row stochastic),
the term $eu^T$ is to add a small constant to every entry of S.

So now the power method is:

$$
v_k= Gv_{k-1}= \alpha S v_{k-1} + (1-\alpha)u
$$
, because v is stochastic.

Now the power method converges at the same rate as $\alpha^k \to 0$

\subsection{Product Recommendation systems}

Insight: Personal Preferences are correlated

\subsubsection{Problem Formulation}

Given: 

\begin{itemize}
\item Users $u\in \{1,...,U\}$
\item Items $i\in\{1,...,M\}$
\item Training set with observed, real-valued preferences for some user-item pair (u,i)
\end{itemize}

Goal: Predict unobserved preferences

Methodology: assume a distribution and update the parameters. Dimensionality reduction



\end{document}
